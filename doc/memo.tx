 _, h, w, c = pool2.get_shape().as_list()


deconv = tf.nn.conv2d_transpose(bottom, weights, output_shape,
                                strides=strides, padding='SAME')

deconv = tf.Print(deconv, [tf.shape(deconv)],
                  message='Shape of %s' % name, summarize=4, first_n=1)


global_step = tf.Variable(0, trainable=False)
starter_learning_rate = 0.1
learning_rate = tf.train.exponential_decay(starter_learning_rate,
                                           global_step,
                                           100, 0.9, staircase=True)
optimizer = tf.train.AdamOptimizer(learning_rate)

#Define your exponentially decaying learning rate
lr = tf.train.exponential_decay(
            learning_rate = initial_learning_rate,
            global_step = global_step,
            decay_steps = decay_steps,
            decay_rate = learning_rate_decay_factor,
            staircase = True)

#Now we can define the optimizer that takes on the learning rate
optimizer = tf.train.AdamOptimizer(learning_rate=lr, epsilon=epsilon)


How to get tensor names of any pre loaed model.
----------------------------------------------

You should first load the graph to the session and from the default graph get all the operations using graph.get_operations() . You can get all the tensors by adding a :0 to the end of op names . Then use graph.get_tensor_by_name(tensor_name) to get the tensor .


-----------------------------------------------------------------------------------
When you train a neural network in tensorflow and save the model at some step using tf.train.Saver(), you get three files namely,
    checkpoint
    A index file
    A meta file
You never get a protobuf file but when you want to freeze your graph you need to pass the protobuf file as input
how to generate the protobuf file in the first place?
------------------------------------------------------

First you need to name the logits layer of your model i.e.

logits = tf.reshape(nn_last_layer, (-1, num_classes), name="adam_logit")

Then save your model like so:

        save_path = saver.save(sess, "model.ckpt")
        print("Model saved in file: %s" % save_path)
        tf.train.write_graph(sess.graph_def, '', 'model_text.pb', True)
        tf.train.write_graph(sess.graph_def, '', 'model_.pb', False)

To load the model back into your tensorflow program you can use:

            import graph_utils as gu

            sess, ops = gu.load_graph('eightbit.pb')
            g = sess.graph
            input_layer = g.get_tensor_by_name('image_input:0')
            keep_prob = g.get_tensor_by_name('keep_prob:0')
            logits = g.get_tensor_by_name('adam_logit:0')


How to get dynamic shapes:
---------------------------
You can feed images into your computational graph, then use sess.run() function to get dynamic shapes.

image_shape, layer3_shape, layer4_shape, layer7_shape = sess.run([
 tf.shape(image_input), tf.shape(layer3_out), tf.shape(layer4_out), tf.shape(layer7_out)], 
feed_dict={input_image: batch_x ... })
print(image_shape, layer3_shape, layer4_shape, layer7_shape)

Then you could get dynamic shapes:
[ 1 160 576 3]
[ 1 20 72 256]
[ 1 10 36 512]
[ 1 5 18 4096]

But if you donâ€™t use sess.run(), you will get static shapes:
?x?x?x256,
?x?x?x512
?x?x?x4092.

layer7_1x1 = conv_1x1(vgg_layer7_out, num_classes)
layer7_1x1 = tf.Print(layer7_1x1, [tf.shape(layer7_1x1)], message= "Shape of layer7_1x1:", summarize=10, first_n=1)

def printShape(layer, message):
# print the dimensions of this layer
  return tf.Print(layer, [tf.shape(layer)], message=message + " shape:", summarize=10, first_n=1)

-----------------------------------------------------------------
        saver = tf.train.import_meta_graph('checkpoints/model3.meta')
        saver.restore(sess, tf.train.latest_checkpoint('checkpoints/.'))
        graph = tf.get_default_graph()
        image_input = graph.get_tensor_by_name("image_input:0")
        keep_prob = graph.get_tensor_by_name("keep_prob:0")

        print("graph names: \n", [n.name for n in graph.as_graph_def().node])

