# required inputs in model_dir:
# -----------------------------
# fcn8s.index
# fcn8s.data-00000-of-00001
# fcn8s.meta
# checkpoint


# 1) Freeze graph (aka model below):
# ----------------------------------

# 2165 ops -> 245 ops
python freeze_model.py --model_dir . --output_node_names Softmax

output: freeze_model.pb (a protobuf file)

# 2) Optimize graph:
# ------------------

# 245 ops -> 209 ops
python /home/phw/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/tools/optimize_for_inference.py --input=frozen_model.pb --output=optimized_graph.pb --frozen_graph=True --input_names=image_input --output_names=Softmax

python load_optimizzed.py

# 3) 8-bis Calculations:
# ----------------------

using tensorflow tool transform_graph
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md

bazel build tensorflow/tools/graph_transforms:transform_graph bazel-bin/tensorflow/tools/graph_transforms/_objs/

/home/philippew/Udacity-term3/tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph

transform_graph \
--in_graph=frozen_graph.pb \
--out_graph=eightbit_graph.pb \
--inputs=image_input \
--outputs=Softmax \
--transforms='
add_default_attributes
remove_nodes(op=Identity, op=CheckNumerics)
fold_constants(ignore_errors=true)
fold_batch_norms
fold_old_batch_norms
fuse_resize_and_conv
quantize_weights
quantize_nodes
strip_unused_nodes
sort_by_execution_order'

The gist is that fold transforms look for subgraphs that always evaluate to to the same result. Then they consolidate each such subgraph into one Constant node.

quantize_weights quantizes values larger than 15 bits. It also adds nodes to convert back to floating point. The quantize_weights transform is mainly for reducing graph size. For the desired quantization computation behaviour weâ€™ll need to use quantize_nodes as well.
